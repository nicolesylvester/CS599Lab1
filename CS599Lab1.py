# -*- coding: utf-8 -*-
"""CS599Lab1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YqxwjKHn6WY0K_KcQJHtBoYE04KCl6X1
"""

import tensorflow as tf
import time
import numpy as np
import matplotlib.pyplot as plt

#### Problem 1 ##########
# Create data
NUM_EXAMPLES = 500

# Convert name to a unique seed
name = "Nicole"
seed = int(''.join(str(ord(char)) for char in name))

tf.random.set_seed(seed)

loss_type = "Hybrid"

patience = 300            # Number of steps to wait before reducing LR
lr_decay_factor = 0.5     # Factor to reduce LR by (e.g., multiply by 0.5)
best_loss = float('inf')  # Initialize best loss as infinity
patience_counter = 0      # Counter to track how long loss has not improved

train_steps = 5000  # Number of training iterations
learning_rate = 0.01  # Step size
base_learning_rate = 0.01

noise_level = 0.5
noise_interval = 500

experiments = {
      "Huber Loss with No Noise": {
        "data_noise": 0.0,  # No input noise
        "weight_noise": 0.0,  # No weight noise
        "lr_noise": 0.0,  # No learning rate noise
        "noise_type": "none",  # No noise type needed
    }
    # "Input Noise (Gaussian 0.5)": {
    #     "data_noise": 0.5,
    #     "weight_noise": 0.0,
    #     "lr_noise": 0.0,
    #     "noise_type": "gaussian"
    # },
    # "Weight Noise (Gaussian 0.5)": {
    #     "data_noise": 0.0,
    #     "weight_noise": 0.5,
    #     "lr_noise": 0.0,
    #     "noise_type": "gaussian"
    # },
    # "Learning Rate Noise (Gaussian 0.5)": {
    #     "data_noise": 0.0,
    #     "weight_noise": 0.0,
    #     "lr_noise": 0.5,
    #     "noise_type": "gaussian"
    # }
}


results = {}

def generate_data(data_noise=0.0, noise_type="gaussian"):
    X = tf.random.normal([NUM_EXAMPLES])
    noise = generate_noise(noise_type=noise_type, size=NUM_EXAMPLES, level=data_noise)
    y = X * 3 + 2 + noise
    return X, y

def generate_noise(noise_type="gaussian", size=NUM_EXAMPLES, level=0.5):
    if noise_type == "gaussian":
        return tf.random.normal([size], mean=0.0, stddev=level)
    elif noise_type == "uniform":
        return tf.random.uniform([size], minval=-level, maxval=level)
    elif noise_type == "laplace":
        return np.random.laplace(loc=0.0, scale=level, size=size)
    elif noise_type == "none":
        return np.zeros(size)
    else:
        raise ValueError("Unknown noise type. Choose 'gaussian', 'uniform', 'laplace', 'salt_and_pepper', or 'none'.")

# Define the linear predictor
def prediction(x, W, b):
    return X * W + b

# Define Mean Squared Error (MSE) Loss
def squared_loss(y, y_predicted):
    return tf.reduce_mean(tf.square(y - y_predicted))

# Define Huber Loss Function
def huber_loss(y, y_predicted, m=1.0):
    error = y - y_predicted
    is_small_error = tf.abs(error) <= m
    small_error_loss = 0.5 * tf.square(error)
    big_error_loss = m * tf.abs(error) - 0.5 * (m**2)
    return tf.reduce_mean(tf.where(is_small_error, small_error_loss, big_error_loss))


def train_model(X, y, data_noise=0.0, weight_noise=0.0, lr_noise=0.0, noise_type="gaussian", experiment_name=""):
  W = tf.Variable(0.0)
  b = tf.Variable(0.0)

  best_loss = float('inf')
  patience_counter = 0
  learning_rate = base_learning_rate

  cpu_times = []
  gpu_times = []
  losses = []

  for i in range(train_steps):
    with tf.device('/CPU:0'):
      cpu_start = time.time()
      with tf.GradientTape() as tape:
          # Forward pass: compute predicted y (yhat)
          yhat = prediction(X, W, b)

          # Compute loss based on the selected loss function
          if loss_type == "MSE":
              loss = squared_loss(y, yhat)
          elif loss_type == "Huber":
            loss = huber_loss(y, yhat, m=1.0)
          elif loss_type == "MAE":
              loss = tf.reduce_mean(tf.abs(yhat - y))
          elif loss_type == "Hybrid":
              # Here, we blend L1 and L2 losses.
              # alpha controls the trade-off between L1 (MAE) and L2 (MSE) components.
              alpha = 0.5
              loss = tf.reduce_mean(alpha * tf.abs(yhat - y) + (1 - alpha) * tf.square(yhat - y))
          else:
              raise ValueError("Unknown loss type selected. Choose 'MSE', 'MAE', or 'Hybrid'.")

      # Compute gradients of loss with respect to W and b
      dW, db = tape.gradient(loss, [W, b])
      cpu_end = time.time()
      cpu_times.append(cpu_end - cpu_start)

    with tf.device('/GPU:0'):
      gpu_start = time.time()

      # Add noise to weights every N epochs
      if i % noise_interval == 0 and weight_noise > 0:
          W.assign_add(generate_noise(noise_type=noise_type, size=1, level=weight_noise)[0])
          b.assign_add(generate_noise(noise_type=noise_type, size=1, level=weight_noise)[0])


      # Update parameters using gradient descent
      W.assign_sub(learning_rate * dW)
      b.assign_sub(learning_rate * db)

      if i % noise_interval == 0 and lr_noise > 0:
        learning_rate += generate_noise(noise_type=noise_type, size=1, level=lr_noise)[0]
        learning_rate = max(learning_rate, 1e-6)  # Avoid negative or zero LR


      # --- Learning Rate Scheduling ---
      # If the loss improves, reset the patience counter; otherwise, increase it.
      current_loss = loss.numpy()
      if current_loss < best_loss:
          best_loss = current_loss
          patience_counter = 0
      else:
          patience_counter += 1

      # If the loss hasn't improved for 'patience' steps, reduce the learning rate.
      if patience_counter >= patience:
          learning_rate *= lr_decay_factor
          print(f"Reducing learning rate to {learning_rate:.6f} at step {i}")
          patience_counter = 0  # Reset the counter after reducing LR

      gpu_end = time.time()
      gpu_times.append(gpu_end - gpu_start)

      losses.append(loss.numpy())

      # Print training progress every 500 steps
      if i % 500 == 0:
          print(f"{experiment_name} | Step {i}, Loss: {current_loss:.4f}, W: {W.numpy():.4f}, b: {b.numpy():.4f}")

  print(f"\nFinal Model: W = {W.numpy():.4f}, b = {b.numpy():.4f}, Final Loss: {loss.numpy():.4f}")
  print(f"Average CPU Time per Epoch: {np.mean(cpu_times):.6f} seconds")
  print(f"Average GPU Time per Epoch: {np.mean(gpu_times):.6f} seconds")

  return W, b, losses, cpu_times, gpu_times

def plot_results(experiment_name, losses, cpu_times, gpu_times):
    # Plot 1: Training Loss per Epoch
    plt.figure(figsize=(10, 6))
    plt.plot(losses, label='Training Loss', color='red')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title(f'Training Loss per Epoch [{experiment_name}]')
    plt.legend()
    plt.grid(True)
    plt.show()

    # Plot 2: GPU vs CPU Time per Epoch
    plt.figure(figsize=(10, 6))
    plt.plot(cpu_times, label='CPU Time per Epoch', linestyle='--', color='blue')
    plt.plot(gpu_times, label='GPU Time per Epoch', linestyle='-', color='green')
    plt.xlabel('Epoch')
    plt.ylabel('Time (seconds)')
    plt.title(f'GPU vs CPU Execution Time per Epoch [{experiment_name}]')
    plt.legend()
    plt.grid(True)
    plt.show()
# 5. Run experiments
for experiment_name, config in experiments.items():
    print(f"\nRunning Experiment: {experiment_name}")
    X, y = generate_data(data_noise=config["data_noise"], noise_type=config["noise_type"])
    W, b, losses, cpu_times, gpu_times = train_model(
        X, y,
        data_noise=config["data_noise"],
        weight_noise=config["weight_noise"],
        lr_noise=config["lr_noise"],
        noise_type=config["noise_type"],
        experiment_name=experiment_name
    )

    plot_results(experiment_name, losses, cpu_times, gpu_times)


########### Problem 2 ###########
import os
os.environ['TF_CPP_MIN_LOG_LEVEL']='2'

import numpy as np
import tensorflow as tf
import time
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
from tensorflow.keras.datasets import fashion_mnist
import random

# Define paramaters for the model
learning_rate = 0.001
batch_size = 64
n_epochs = 20

# Convert name to a unique seed
name = "Nicole"
seed = int(''.join(str(ord(char)) for char in name))

tf.random.set_seed(seed)

# Step 1: Read in data

#Create dataset load function [Refer fashion mnist github page for util function]
#Create train,validation,test split
#train, val, test = utils.read_fmnist(fmnist_folder, flatten=True)

# fashion_mnist = tf.keras.datasets.fashion_mnist

# Normalize pixel values to the range [0, 1] and cast to float32
(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()
train_images = (train_images / 255.0).astype(np.float32)
test_images = (test_images / 255.0).astype(np.float32)

# Manual validation split (10% for validation)
val_split = 0.2
num_val = int(train_images.shape[0] * val_split)
val_images = train_images[:num_val]
val_labels = train_labels[:num_val]
train_images_split = train_images[num_val:]
train_labels_split = train_labels[num_val:]

print(f"Training set: {train_images.shape}, Validation set: {val_images.shape}, Test set: {test_images.shape}")

# Step 2: Create datasets and iterator
# create training Dataset and batch it
train_data = tf.data.Dataset.from_tensor_slices((train_images, train_labels.astype('int32')))
train_data = train_data.shuffle(buffer_size=10000,seed=seed).batch(batch_size)

# Create the validation dataset and batch it (no need to shuffle validation data)
val_data = tf.data.Dataset.from_tensor_slices((val_images, val_labels.astype('int32')))
val_data = val_data.batch(batch_size)

# create testing Dataset and batch it
#############################
########## TO DO ############
#############################
# Create the testing dataset and batch it
test_data = tf.data.Dataset.from_tensor_slices((test_images, test_labels.astype('int32')))
test_data = test_data.batch(batch_size)


# create one iterator and initialize it with different datasets
# iterator = tf.data.Iterator.from_structure(train_data.output_types,
#                                            train_data.output_shapes)
# img, label = iterator.get_next()

# train_init = iterator.make_initializer(train_data)	# initializer for train_data
# test_init = iterator.make_initializer(test_data)	# initializer for train_data

# Step 3: create weights and bias
# w is initialized to random variables with mean of 0, stddev of 0.01
# b is initialized to 0
# shape of w depends on the dimension of X and Y so that Y = tf.matmul(X, w)
# shape of b depends on Y
#############################
########## TO DO ############
#############################
w = tf.Variable(tf.zeros([28 * 28, 10]))
b = tf.Variable(tf.zeros([10]))

# Step 4: build model
# the model that returns the logits.
# this logits will be later passed through softmax layer
#############################
########## TO DO ############
#############################
# Define the logistic regression model using a custom tf.Module class
class LogisticRegressionModel(tf.Module):
    def __init__(self):
        super().__init__()
        self.W = tf.Variable(tf.zeros([28 * 28, 10]))
        self.b = tf.Variable(tf.zeros([10]))

    def __call__(self, x):
        # Cast input to float32 to ensure consistency.
        x = tf.cast(x, tf.float32)
        x = tf.reshape(x, [-1, 28 * 28])
        logits = tf.matmul(x, self.W) + self.b
        return tf.nn.softmax(logits)


# Step 5: define loss function
# use cross entropy of softmax of logits as the loss function
#############################
########## TO DO ############
#############################
def compute_loss(model, images, labels, lambda_reg=0.0):
    predictions = model(images)
    # Compute the standard sparse categorical cross-entropy loss.
    ce_loss = tf.reduce_mean(tf.losses.sparse_categorical_crossentropy(labels, predictions))
    # L2 loss on weights (do not regularize bias).
    l2_loss = tf.nn.l2_loss(model.W)  # Sum of squares divided by 2
    return ce_loss + lambda_reg * l2_loss

def train_step(model, images, labels, optimizer, lambda_reg=0.0):
    with tf.GradientTape() as tape:
        loss = compute_loss(model, images, labels, lambda_reg)
    # Compute gradients with respect to the model's variables.
    grads = tape.gradient(loss, [model.W, model.b])
    # Update the variables using the optimizer.
    optimizer.apply_gradients(zip(grads, [model.W, model.b]))
    return loss
# Step 6: define optimizer
# using Adam Optimizer with pre-defined learning rate to minimize loss
#############################
########## TO DO ############
#############################
optimizer = tf.optimizers.Adam(learning_rate=learning_rate)
optimizers_to_try = {
    "SGD": tf.optimizers.SGD(learning_rate=0.5),
    "Adam": tf.optimizers.Adam(learning_rate=0.001),
    "RMSprop": tf.optimizers.RMSprop(learning_rate=0.001)
}

# Step 7: calculate accuracy with test set
# preds = tf.nn.softmax(logits)
# correct_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(label, 1))
# accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))
train_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()
val_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()

def loss_fn(y_true, y_pred):
    return tf.reduce_mean(tf.losses.sparse_categorical_crossentropy(y_true, y_pred))

def train_step(model, images, labels, optimizer, lambda_reg=0.0):
    with tf.GradientTape() as tape:
        loss = compute_loss(model, images, labels, lambda_reg)
    # Compute gradients with respect to the model's variables.
    grads = tape.gradient(loss, [model.W, model.b])
    # Update the variables using the optimizer.
    optimizer.apply_gradients(zip(grads, [model.W, model.b]))
    return loss

#Step 8: train the model for n_epochs times
def train_model(optimizer, lambda_reg=0.0, num_epochs=20, batch_size=64):
    # Create a new model instance (fresh initialization)
    model = LogisticRegressionModel()
    # Prepare tf.data datasets for training and validation
    train_dataset = tf.data.Dataset.from_tensor_slices((train_images_split, train_labels_split))
    train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)
    val_dataset = tf.data.Dataset.from_tensor_slices((val_images, val_labels))
    val_dataset = val_dataset.batch(batch_size)
    # Lists to record metrics for each epoch.
    history = {
        "train_loss": [],
        "val_loss": [],
        "train_accuracy": [],
        "val_accuracy": []
    }
    # For accuracy computation
    train_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()
    val_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()

    for epoch in range(1, num_epochs + 1):
        # Reset metrics at the start of each epoch.
        train_acc_metric.reset_state()
        val_acc_metric.reset_state()

        # Training loop
        epoch_losses = []
        for batch_images, batch_labels in train_dataset:
            loss = train_step(model, batch_images, batch_labels, optimizer, lambda_reg)
            epoch_losses.append(loss.numpy())
            # Update training accuracy.
            predictions = model(batch_images)
            train_acc_metric.update_state(batch_labels, predictions)

        # Compute average training loss over epoch.
        train_loss = np.mean(epoch_losses)
        train_accuracy = train_acc_metric.result().numpy()

        # Validation loop
        val_losses = []
        for batch_images, batch_labels in val_dataset:
            loss = compute_loss(model, batch_images, batch_labels, lambda_reg)
            val_losses.append(loss.numpy())
            predictions = model(batch_images)
            val_acc_metric.update_state(batch_labels, predictions)
        val_loss = np.mean(val_losses)
        val_accuracy = val_acc_metric.result().numpy()

        # Save metrics
        history["train_loss"].append(train_loss)
        history["val_loss"].append(val_loss)
        history["train_accuracy"].append(train_accuracy)
        history["val_accuracy"].append(val_accuracy)

        print(f"Epoch {epoch:02d}: "
              f"Train Loss = {train_loss:.4f}, Train Acc = {train_accuracy:.4f}, "
              f"Val Loss = {val_loss:.4f}, Val Acc = {val_accuracy:.4f}")
    return model, history



#Step 9: Get the Final test accuracy
def evaluate_model_on_test_set(model, test_data):
    test_loss = tf.keras.metrics.Mean()
    test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()

    for images, labels in test_data:
        logits = model(images)
        loss = loss_fn(labels, logits)
        test_loss.update_state(loss)
        test_accuracy.update_state(labels, logits)

    print(f"\nTest Loss: {test_loss.result():.4f}, Test Accuracy: {test_accuracy.result() * 100:.2f}%")

#Step 10: Helper function to plot images in 3*3 grid
#You can change the function based on your input pipeline
img_shape = (28, 28)
def plot_images(images, y, yhat=None):
    assert len(images) == len(y) == 9

    # Create figure with 3x3 sub-plots.
    fig, axes = plt.subplots(3, 3)
    fig.subplots_adjust(hspace=0.3, wspace=0.3)

    for i, ax in enumerate(axes.flat):
        # Plot image.
        ax.imshow(images[i].reshape(img_shape), cmap='binary')

        # Show true and predicted classes.
        if yhat is None:
            xlabel = "True: {0}".format(y[i])
        else:
            xlabel = "True: {0}, Pred: {1}".format(y[i], yhat[i])

        ax.set_xlabel(xlabel)

        # Remove ticks from the plot.
        ax.set_xticks([])
        ax.set_yticks([])
    plt.show()

#Second plot weights

def plot_weights(model, w=None):
    # Get the values for the weights from the TensorFlow variable.
    #TO DO ####

    # Get the lowest and highest values for the weights.
    # This is used to correct the colour intensity across
    # the images so they can be compared with each other.
    #TO DO## obtains these value from W

    w = model.W.numpy()  # Access weights from the model
    w_min, w_max = np.min(w), np.max(w)

    # Create figure with 3x4 sub-plots,
    # where the last 2 sub-plots are unused.
    fig, axes = plt.subplots(3, 4)
    fig.subplots_adjust(hspace=0.3, wspace=0.3)

    for i, ax in enumerate(axes.flat):
        # Only use the weights for the first 10 sub-plots.
        if i<10:
            # Get the weights for the i'th digit and reshape it.
            # Note that w.shape == (img_size_flat, 10)
            image = w[:, i].reshape(img_shape)

            # Set the label for the sub-plot.
            ax.set_xlabel("Weights: {0}".format(i))

            # Plot the image.
            ax.imshow(image, vmin=w_min, vmax=w_max, cmap='seismic')

        # Remove ticks from each sub-plot.
        ax.set_xticks([])
        ax.set_yticks([])

    # Ensure the plot is shown correctly with multiple plots
    # in a single Notebook cell.
    plt.show()

history_dict = {}
lambda_reg = 0.001
num_epochs = 20

for opt_name, opt in optimizers_to_try.items():
    print(f"Training with {opt_name} optimizer (lambda_reg={lambda_reg})")
    # Train a new model for each optimizer.
    model, history = train_model(opt, lambda_reg=lambda_reg, num_epochs=num_epochs, batch_size=64)
    history_dict[opt_name] = history

# Visualize sample images from the test set with predictions
sample_images = test_images[:9]
sample_labels = test_labels[:9]
predictions = model(sample_images)
predicted_labels = np.argmax(predictions.numpy(), axis=1)

plot_images(sample_images, sample_labels, predicted_labels)

# Visualize the weights of the trained model
plot_weights(model)